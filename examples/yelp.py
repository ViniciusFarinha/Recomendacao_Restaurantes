# -*- coding: utf-8 -*-
"""Yelp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CrNaMwIwAB24VUV2NDTbHaqAjkVMFBii

# Purpose

O objetivo do projeto é recomendar restaurantes para os usuários que interagiram e deram reviews no site da Yelp.

A ideia é extrair os dados do site e criar um algoritmo de recomendação ppara esses usuários.

# Pipeline

                           ┌─────────────────┐
                           │ Obter Dados de  │
                           │ API da Yelp     │
                           └─────────────────┘
                                    │
                                    ▼
                           ┌─────────────────┐
                           │ Data Preparation│
                           │                 │
                           └─────────────────┘
                                    │
                                    ▼
                           ┌─────────────────┐
                           │     Feature     │
                           │   Engineering   │
                           │                 │
                           └─────────────────┘
                                    │
                                    ▼
                           ┌─────────────────┐
                           │ Pre-processing  │
                           └─────────────────┘
                                    │
                                    ▼
                           ┌─────────────────┐
                           │     Machine     │
                           │     Learning    │
                           └─────────────────┘
                                    │
                                    ▼
                           ┌─────────────────┐
                           │     Results     │
                           └─────────────────┘

A pipeline é composta pelas seguintes etapas:

<br>
<br>

1.   Obter os dados do site a partir da API da Yelp. Tanto dos restaurantes quanto das reviews. O que pode ser trabalhoso devido ao limite diário de requisições que podem ser feitas (500)

2.   Preparar os dados concatenando os dados que foram extraídos, renomeando colunas e realizando ajustes de dtypes e strings.

3.   Feature Engineering. Onde são criadas colunas novas com  base nas necessidades, além de aplicar conceitos matemáticos para aliviar viés de distribuição.

4.   Machine Learning para realizar o algoritmo de recomendação: ainda há definir.

5.   Resultados do trabalho realizado

# Imports
"""

# Extraction
import requests

# Wrangling
import pandas as pd
import numpy as np
from pandas import json_normalize

#Viz
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import Image

# Pre-Processing

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Machine Learning

import networkx as nx
from sklearn.cluster import KMeans
from textblob import TextBlob
from collections import Counter

# Metrics

from sklearn.metrics import silhouette_score

# Serial
import pickle




sns.set(style="whitegrid")

"""# Data Extraction"""

# Adicionando url e key ás variáveis

url = 'https://api.yelp.com/v3/businesses/search'
key = 'Hzj0_Eb2tv3dvew715HDLGjuv157Q_8wPX1x1gAPXJYbshXhp1j9-UJpQ2fW_9GBidnXe0aN3N4unuT41BJya_z6_s8CRflzXCyKRDouNpNRMSc3sDbA6XNp8oFFZXYx'

headers = {
    'Authorization': 'Bearer %s' % key
}

# Definindo os parâmetros da pesquisa
term = 'restaurantes'
location = 'Rio de Janeiro'
limit = 50

# Lista vazia para armazenar os resultados
results = []

# Solicitações para obter todos os resultados
for offset in range(0, 1000, limit):
    params = {
        'term': term,
        'location': location,
        'limit': 50,
        'offset': offset,
    }
    response = requests.get('https://api.yelp.com/v3/businesses/search', headers=headers, params=params)
    data = response.json()
    results.extend(data['businesses'])

# Convertendo os resultados em um DataFrame do pandas
df = pd.DataFrame(results)

# Exibindo o dataframe
df

# Salvando o DataFrame em um arquivo CSV
df.to_csv('meu_arquivo.csv', index=False)


# Load dos dados do df já extraidos

df= pd.read_csv('meu_arquivo.csv')

# Para contornar a limitação da API, dividi o dataset em 4 partes, e assim vou pegando as partes em 4 dias conssecutivos
df1 = df.iloc[:200] # 08/11/2023
df2 = df.iloc[200:500]
df3 = df.iloc[500:1000]

"""## Df1_reviews"""

# Meu desejo é extrair um dataset sobre as reviews de cada restaurante para poder linkar as informações e fazer um algoritmo de recomendação por usuário, mas as requisições diárias estão em mais de 500, o que excede o limite
reviews1 = []
id_restaurants = []

for restaurant_id in df1['id']:
    url_reviews  = f"https://api.yelp.com/v3/businesses/{restaurant_id}/reviews?limit=50&sort_by=yelp_sort"
    response_review = requests.get(url_reviews , headers=headers)
    data_reviews = response_review.json().get('reviews', [])
    reviews1.extend(data_reviews)
    id_restaurants.extend([restaurant_id] * len(data_reviews))
# Converta os resultados em um DataFrame do pandas
df1_reviews = pd.DataFrame(reviews1)

# Adicionando a coluna de ID dos restaurantes
df1_reviews['restaurant_id'] = id_restaurants

# Exibindo o DataFrame
df1_reviews

# Abrindo colunas de user (que estão em dicionario)

user = json_normalize(df1_reviews['user'])
user

df1_reviews = pd.concat([df1_reviews,user],axis =1)
df1_reviews.drop(columns= ['user','profile_url','image_url'],inplace = True)

df1_reviews.rename(columns = {'id':'user_id'})

# Abrindo colunas de user (que estão em dicionario)

user = json_normalize(df1_reviews['user'])
user

df1_reviews = pd.concat([df1_reviews,user],axis =1)
df1_reviews.drop(columns= ['user','profile_url','image_url'],inplace = True)

df1_reviews.rename(columns = {'id':'user_id'})
# Tratamento parcial dos dados

# Rename

df1_reviews.rename(columns = {'id': 'review_id',
                              'url':'url_id',
                              'rating':'review_rate'

                              },inplace = True)

# Salvando o DataFrame em um arquivo CSV
df1_reviews.to_csv('df1_reviews.csv', index=False)



"""## Df2_reviews"""

reviews2 = []

id_restaurants = []

for restaurant_id in df2['id']:
   url_reviews  = f"https://api.yelp.com/v3/businesses/{restaurant_id}/reviews?limit=50&sort_by=yelp_sort"
   response_review = requests.get(url_reviews,headers= headers)
   data_reviews = response_review.json().get('reviews', [])
   reviews2.extend(data_reviews)
   id_restaurants.extend([restaurant_id] * len(data_reviews))

# Transformar em dataframe

df2_reviews = pd.DataFrame(reviews2)

# Adicionando a coluna de ID dos restaurantes
df2_reviews['restaurant_id'] = id_restaurants

# Exibindo o DataFrame
df2_reviews

# Salvando o DataFrame em um arquivo CSV
df2_reviews.to_csv('df2_reviews.csv', index=False)


"""## Df3_reviews"""

reviews3 = []

id_restaurants = []

for restaurant_id in df3['id']:
   url_reviews  = f"https://api.yelp.com/v3/businesses/{restaurant_id}/reviews?limit=50&sort_by=yelp_sort"
   response_review = requests.get(url_reviews,headers= headers)
   data_reviews = response_review.json().get('reviews', [])
   reviews3.extend(data_reviews)
   id_restaurants.extend([restaurant_id] * len(data_reviews))

# Transformar em dataframe

df3_reviews = pd.DataFrame(reviews3)

# Adicionando a coluna de ID dos restaurantes
df3_reviews['restaurant_id'] = id_restaurants

# Exibindo o DataFrame
df3_reviews

# Salvando o DataFrame em um arquivo CSV
df3_reviews.to_csv('df3_reviews.csv', index=False)



"""# Data Cleaning - DF"""

df = pd.read_csv('meu_arquivo.csv')

# Entendendo valores NaN
df.isna().sum()

# Retificando valores vazios para apenas um $

df['price'] = df['price'].fillna('$')
df.isna().sum()

# Retirando colunas que não fazem sentido para a análise

df.columns

#Convertendo a coluna price para valores numéricos
df['price'] = df['price'].apply(lambda x: len(x))

df.drop(axis=1 , columns =['alias','image_url','url','transactions','coordinates','display_phone','phone'],inplace= True)

df['categories']

import json

# Função para extrair a categoria do formato JSON para duas colunas separadas: 'alias' e 'title'
def extract_categories(row):
    try:
        # Converte a string JSON em um objeto Python
        categories_list = json.loads(row.replace("'", "\""))

        # Se a lista não estiver vazia, extrai 'alias' e 'title' do primeiro elemento
        if categories_list:
            return categories_list[0]['alias'], categories_list[0]['title']
        return None, None
    except json.JSONDecodeError:
        return None, None

# Aplicar a função ao DataFrame
df[['category_alias', 'category_title']] = df['categories'].apply(lambda x: pd.Series(extract_categories(x) if pd.notnull(x) else (None, None)))

# Visualizar as primeiras linhas do DataFrame modificado
df.head()

df.drop(columns=['categories','category_alias','location'],inplace = True)
df.rename(columns={'category_title':'category'},inplace = True)

df['is_closed'] = df['is_closed'].astype(int)

df

df['category'].unique()

# Dicionário de mapeamento
mapping = {
    'Brazilian': 'Brazilian',
    'Modern European': 'European',
    'Cocktail Bars': 'Beverages',
    'Steakhouses': 'Type of Restaurant',
    'Rodizios': 'Rodizios',
    'Pizza': 'Type of Restaurant',
    'Seafood': 'Fish',
    'Bars': 'Beverages',
    'Vegan': 'Vegan',
    'Vegetarian': 'Vegan',
    'Japanese': 'Asian',
    'Sushi Bars': 'Fish',
    'Local Flavor': 'Cuisine',
    'Sandwiches': 'Snack',
    'Buffets': 'Type of Restaurant',
    'Breakfast & Brunch': 'Snack',
    'Salad': 'Fitness',
    'Mediterranean': 'Asian',
    'Tapas Bars': 'Type of Restaurant',
    'Gastropubs': 'Type of Restaurant',
    'International': 'Cuisine',
    'Italian': 'European',
    'Northern Brazilian': 'Brazilian',
    'French': 'European',
    'Arabic': 'Asian',
    'Diners': 'Type of Restaurant',
    'Bistros': 'European',
    'American': 'Cuisine',
    'Burgers': 'Snack',
    'Fast Food': 'Snack',
    'Chicken Wings': 'Chicken',
    'Pool Halls': 'Beverages',
    'Restaurants': 'Type of Restaurant',
    'Argentine': 'Latin',
    'Beer, Wine & Spirits': 'Beverages',
    'Portuguese': 'European',
    'Dive Bars': 'Beverages',
    'Indian': 'Asian',
    'Wine Bars': 'Beverages',
    'Thai': 'Asian',
    'Polish': 'European',
    'Chinese': 'Asian',
    'Spanish': 'European',
    'Cafes': 'Beverages',
    'Venues & Event Spaces': 'Entertainment',
    'German': 'European',
    'Fondue': 'Desserts',
    'Northeastern Brazilian': 'Brazilian',
    'Creperies': 'Type of Restaurant',
    'Delicatessen': 'Type of Restaurant',
    'Kebab': 'Cuisine',
    'Sports Bars': 'Entertainment',
    'Bakeries': 'Desserts',
    'Music Venues': 'Entertainment',
    'Organic Stores': 'Fitness',
    'Food Delivery Services': 'Snack',
    'Brasseries': 'Beverages',
    'Desserts': 'Desserts',
    'Chicken Shop': 'Chicken',
    'Asian Fusion': 'Asian',
    'Food Stands': 'Snack',
    'Beer Bar': 'Beverages',
    'Mexican': 'Latin',
    'Tex-Mex': 'Cuisine',
    'Brazilian Empanadas': 'Brazilian',
    'Peruvian': 'Latin',
    'Juice Bars & Smoothies': 'Beverages',
    'Kosher': 'Cuisine',
    'Izakaya': 'Asian',
    'Breweries': 'Beverages',
    'Lounges': 'Type of Restaurant',
    'Latin American': 'Latin',
    'Acai Bowls': 'Desserts',
    'Health Markets': 'Fitness',
    'Tapas/Small Plates': 'Type of Restaurant',
    'Food Trucks': 'Snack',
    'Rotisserie Chicken': 'Chicken',
    'Coffee & Tea': 'Beverages',
    'Noodles': 'Asian',
    'Rafting/Kayaking': 'Entertainment',
    'Korean': 'Asian',
    'Gluten-Free': 'Fitness',
    'Soup': 'Fitness',
    'Turkish': 'European',
    'Hot Dogs': 'Snack',
    'Central Brazilian': 'Brazilian',
    'Greek': 'European',
    'Pubs': 'Beverages',
    'Delis': 'Type of Restaurant',
    'Ice Cream & Frozen Yogurt': 'Desserts',
    'Surfing': 'Entertainment'
}

# Substituir as categorias em 'title' pelas categorias mais amplas
df['category'] = df['category'].replace(mapping)

df['category'].value_counts()

# Checkpoint

# Salvando o DataFrame em um arquivo CSV
df.to_csv('df.csv', index=False)



"""# Data Cleaning - reviews"""

reviews1_df = pd.read_csv('/content/df1_reviews.csv')
reviews2_df = pd.read_csv('/content/df2_reviews.csv')
reviews3_df = pd.read_csv('/content/df3_reviews.csv')
df = pd.read_csv('meu_arquivo.csv')

# reviews1_df

reviews1_df.drop(columns= ['user'],inplace = True)

reviews1_df.rename(columns = {'id':'user_id'},inplace = True)
# Tratamento parcial dos dados


reviews1_df

# reviews2_df

import ast
user2 = reviews2_df['user'].tolist()
user2
# Converter as strings que representam dicionários em dicionários reais
user_dicts = [ast.literal_eval(user) if isinstance(user, str) else None for user in user2]

# Agora você pode usar json_normalize nestes dicionários
user_df = pd.json_normalize(user_dicts)

reviews2_df = pd.concat([reviews2_df,user_df],axis =1)
reviews2_df.drop(columns= ['user','profile_url','image_url'],inplace = True)

reviews2_df.rename(columns = {'id':'user_id'},inplace = True)
# Tratamento parcial dos dados

# Rename

reviews2_df.rename(columns = {'id': 'review_id',
                              'url':'url_id',
                              'rating':'review_rate'

                              },inplace = True)
reviews2_df

# reviews3_df

user3 = reviews3_df['user'].tolist()
user3

# Converter as strings que representam dicionários em dicionários reais
user_dicts3 = [ast.literal_eval(user) if isinstance(user, str) else None for user in user3]

# Agora você pode usar json_normalize nestes dicionários
user_df3 = pd.json_normalize(user_dicts3)

reviews3_df = pd.concat([reviews3_df,user_df3],axis =1)
reviews3_df.drop(columns= ['user','profile_url','image_url'],inplace = True)


# Tratamento parcial dos dados

# Rename

reviews3_df.rename(columns = {'id': 'review_id',
                              'url':'url_id',
                              'rating':'review_rate'

                              },inplace = True)
reviews3_df

"""## Load e concat"""

reviews1_df = pd.read_csv('/content/reviews1_df.csv')
reviews2_df = pd.read_csv('/content/reviews2_df.csv')
reviews3_df = pd.read_csv('/content/reviews3_df.csv')

# Concatenar os DataFrames

reviews3_df.rename(columns = {'user_id':'review_id','user_id.1':'user_id'},inplace = True)
reviews2_df.rename(columns = {'user_id':'review_id','user_id.1':'user_id'},inplace = True)

reviews2_df.isna().sum()

reviews1_df.isna().sum()

reviews3_df.isna().sum()

# Concatenar os DataFrames
all_reviews_df = pd.concat([reviews1_df, reviews2_df, reviews3_df], ignore_index=True)
all_reviews_df.isna().sum()

all_reviews_df

# CHeckpoint
# Salvando o DataFrame em um arquivo CSV
all_reviews_df.to_csv('all_reviews_df.csv', index=False)



all_reviews_df.drop(columns = ['url_id','time_created','name'],inplace = True)

all_reviews_df

# Concatenating the dataframes while ignoring the index
try:
    all_reviews_df = pd.concat([reviews1_df, reviews2_df, reviews3_df], ignore_index=True)
    concatenation_success = True
except Exception as e:
    concatenation_error = e
    concatenation_success = False

# Check if the concatenation was successful or if there was an error
concatenation_success, concatenation_error if not concatenation_success else "No Error"

# Concatenando os dataframes
all_reviews_df = pd.concat([reviews1_df, reviews2_df, reviews3_df], ignore_index=True, axis=0)
# Realizando o merge com o dataframe dos restaurantes
merged_df = pd.merge(df, all_reviews_df, left_on='id', right_on='restaurant_id', how='left')
merged_df

merged_df.isna().sum()

# Lista de todos os DataFrames
dfs = [reviews1_df, reviews2_df, reviews3_df]

# Inicializando um DataFrame vazio
all_reviews_df = pd.DataFrame()

# Iterando sobre a lista e concatenando
for df in dfs:
    df_reset = df.reset_index(drop=True)
    all_reviews_df = pd.concat([all_reviews_df, df_reset], ignore_index=True)

all_reviews_df = pd.concat([reviews1_df, reviews2_df], ignore_index=True, axis=0)

merged_df.isna().sum()

merged_df.drop(axis = 1, columns = ['image_url','url_x','transactions','url_id','url_y'],inplace = True)
merged_df

"""## Price"""

merged_df.dtypes

# Retificando valores vazios para apenas um $

merged_df['price'] = merged_df['price'].fillna('$')
merged_df.isna().sum()

#Convertendo a coluna price para valores numéricos
merged_df['price'] = merged_df['price'].apply(lambda x: len(x))
merged_df['price']

# Melhorando a visualização do dataset renomeando as colunas
merged_df.rename( columns = {'id_x': 'restaurant_id',
    'alias': 'restaurant_alias',
    'name_x': 'restaurant_name',
    'categories': 'restaurant_categories',
    'rating_x': 'average_rating',
    'review_id': 'review_id',
    'text': 'review_text',
    'review_rate': 'review_rating',
    'time_created': 'review_time',
    'user': 'review_user_info',
    'restaurant_id': 'matched_restaurant_id',
    'id_y': 'review_dataset_id',
    'name_y': 'review_user_name',
    'rating_y': 'individual_review_rating'

}, inplace = True)

merged_df.columns

# drop de colunas

merged_df.drop(axis = 1, columns = ['restaurant_alias',
                                    'phone',
                                    'display_phone',
                                    'coordinates',
                                    'location',
                                    'review_id',
                                    'review_user_info',
                                    'matched_restaurant_id',
                                    'review_dataset_id',
                                    'review_user_name'], inplace = True)

merged_df

reviews3_df['user']

"""# Pre-Processing"""

df = pd.read_csv('df.csv')
df

all_reviews_df

# concat
combined_final_df = pd.merge(df, all_reviews_df, left_on='id', right_on='restaurant_id', how='left')
combined_final_df

combined_final_df.isna().sum()

"""Devo optar por tirar todos os restaurantes que não tiveram reviews pois isto atrapalharia os algoritmos de machine learning"""

combined_final_df = combined_final_df.dropna(subset =['review_id'])
combined_final_df

"""# Machine Learning

## Análise de sentimentos
"""

combined_final_df = pd.read_csv('/content/combined_final_df.csv')

# Função para calcular o sentimento do texto
def calcular_sentimento(texto):
    try:
        return TextBlob(texto).sentiment.polarity
    except:
        return np.nan

# Aplicar a função de análise de sentimento na coluna 'text'
combined_final_df['sentiment'] = combined_final_df['text'].apply(calcular_sentimento)

# Visualizar as primeiras linhas com a coluna de sentimento adicionada
combined_final_df.head()

# CHeckpoint
# Salvando o DataFrame em um arquivo CSV
combined_final_df.to_csv('combined_df_sentiment.csv', index=False)



# Histograma da Distribuição de Sentimentos
plt.figure(figsize=(10, 6))
sns.histplot(combined_final_df['sentiment'], bins=30, kde=False)
plt.title('Distribuição de Sentimentos das Avaliações')
plt.xlabel('Sentimento')
plt.ylabel('Contagem de Avaliações')
plt.show()

"""*   A maioria das avaliações parece ter um sentimento positivo, como indicado pela concentração de barras no lado positivo do gráfico.
*   Há uma quantidade menor de avaliações com sentimentos negativos, e algumas avaliações parecem ser neutras (próximas a zero).


"""

# Gráfico de Dispersão de Sentimentos vs. Avaliações
plt.figure(figsize=(10, 6))
sns.scatterplot(data=combined_final_df, x='review_rate', y='sentiment', alpha=0.5)
plt.title('Sentimento vs. Avaliação do Usuário')
plt.xlabel('Avaliação do Usuário')
plt.ylabel('Sentimento')
plt.show()

"""*   Podemos observar uma tendência geral de sentimentos mais positivos em avaliações com mais estrelas.

### Piores Restaurantes
"""

# Restaurantes com piores sentimentos

worst_restaurant_reviews = combined_final_df.sort_values(by='sentiment').head(30)
worst_restaurant_reviews

plt.figure(figsize=(13, 7))
sns.countplot(data = worst_restaurant_reviews, x= 'category')
plt.title('Categorias dos 30 Restaurantes com Piores Avaliações (por Sentimento)')
plt.xlabel('Categoria')
plt.ylabel('Contagem')
plt.xticks(rotation=45)
plt.show()

"""O gráfico de barras acima mostra as categorias dos 10 restaurantes com as piores avaliações, baseadas na pontuação de sentimento. Este gráfico é útil para identificar se há alguma tendência ou padrão nas categorias que recebem avaliações particularmente negativas. Temos por exemplo que Brazilian está com a maior quantidade de reviews negativas

### Melhores Restaurantes
"""

best_restaurant_reviews = combined_final_df.sort_values(by='sentiment',ascending=False).head(30)
best_restaurant_reviews

plt.figure(figsize=(13, 7))
sns.countplot(data = best_restaurant_reviews, x= 'category')
plt.title('Categorias dos 30 Restaurantes com Melhores Avaliações (por Sentimento)')
plt.xlabel('Categoria')
plt.ylabel('Contagem')
plt.xticks(rotation=45)
plt.show()

"""As melhores avaliações estão mais comportadas em restaurantes de tipo único e de culinária européia

### Preço e Sentimento

Examinar a relação entre a faixa de preço dos restaurantes e a pontuação de sentimento pode revelar se o preço influencia as expectativas e percepções dos clientes.
"""

# Gráfico de Dispersão para explorar a relação entre Preço e Sentimento
plt.figure(figsize=(10, 6))
sns.scatterplot(data=combined_final_df, x='price', y='sentiment', alpha=0.5)
plt.title('Relação entre Preço e Sentimento')
plt.xlabel('Faixa de Preço')
plt.ylabel('Sentimento')
plt.show()

"""1.   Distribuição de Sentimentos: Parece que há uma distribuição variada de sentimentos em todas as faixas de preço. Isso sugere que o sentimento das avaliações não é consistentemente influenciado pelo preço.

2.   Concentração de Dados: A maioria dos dados parece estar concentrada em faixas de preço mais baixas, o que é comum, já que restaurantes mais acessíveis tendem a ser mais numerosos.


3.   Extremos de Sentimento: Há avaliações com sentimentos extremamente positivos e negativos em todas as faixas de preço, indicando que outros fatores além do preço podem estar influenciando as avaliações dos clientes.

Parece que, enquanto o preço é certamente uma consideração, outros aspectos dos restaurantes podem ter um impacto maior no sentimento das avaliações.

## Clusterização
"""

# Para isso, vou rapidamente transformar a variável price em categórica para melhor exibição dos clusters ao final
combined_final_df['price'] = combined_final_df['price'].replace({   1 :'barato',
                                                                    2 :'mediano',
                                                                    3 : 'caro',
                                                                    4 : 'muito caro'  })

combined_final_df

# salvando o dataframe para recomendacao
df_recomend = combined_final_df

# Selecionando colunas para a análise
numeric_features = ['review_count', 'rating','sentiment']
categorical_features = ['category', 'price']

# Removendo colunas que não serão utilizadas
df_cluster = combined_final_df[numeric_features + categorical_features]

df_cluster

# Pipeline para transformar dados
# Normalização para características numéricas e codificação one-hot para características categóricas
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# Aplicando o pré-processamento
df_transformed = preprocessor.fit_transform(df_cluster)

# Visualizando o shape dos dados transformados
df_transformed.shape

# Determinando a quantidade ótima de clusters usando o método do cotovelo
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(df_transformed)
    wcss.append(kmeans.inertia_)

# Plotando o gráfico do método do cotovelo
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o')
plt.title('Método do Cotovelo')
plt.xlabel('Número de Clusters')
plt.ylabel('WCSS')
plt.show()

# Aplicando o fit do kmeans
kmeans = KMeans(n_clusters=4,random_state = 42)
kmeans.fit(df_transformed)

# Adicionando os clusters ao dataframe
combined_final_df['cluster'] = kmeans.labels_

combined_final_df

# Viz
plt.figure(figsize=(10, 6))
sns.scatterplot(data=combined_final_df, x='rating', y='sentiment', hue='cluster', palette='viridis', alpha=0.6)
plt.title('Clustering of Restaurants by Rating and Sentiment')
plt.xlabel('User Rating (Stars)')
plt.ylabel('Sentiment')
plt.legend(title='Cluster')
plt.show()

combined_final_df.groupby('cluster').mean()

# Criando df baseado no dataframe anterior, para fazer análise final dos clusters
df = combined_final_df
df.drop(columns = ['id', 'name','is_closed', 'review_id', 'text','restaurant_id', 'user_id'],inplace = True)

df

# Criando for que passa por todas as colunas e calcula quanto cada quantidade representa do todo
for col in df.drop('cluster', axis=1).columns:
    for cl in np.sort(df.cluster.unique()):
        if df.dtypes[col] == object:
            vc = 100 * df.loc[df.cluster == cl, col].value_counts() / (df.cluster == cl).sum()
            for cat, cnt in vc.reset_index().values:
                print(f'{cl:d}; {col} {str(cat):s}: {cnt:5.2f}%'.replace(".", ","))
        else:
            print(f'{cl:4}; {col}; {df.loc[df.cluster == cl, col].mean():.2f}'.replace('.', ','))

# E outro loop para colunas do dataframe completo:
for col in df.drop('cluster', axis=1).columns:
    if df.dtypes[col] == object:
        vc = 100 * df.loc[:, col].value_counts() / df.shape[0]
        for cat, cnt in vc.reset_index().values:
            print(f'All; {col} {str(cat):s}; {cnt:5.2F}%'.replace('.', ','))

# Plotando interpretação dos clusters

Image('Clusters_image.png')

"""## Recommendation Algorithm

### Colaborative Filtering - Co-Visitation
"""



df_recomend = pd.read_csv('/content/combined_final_df.csv')

df_recomend

df_recomend['user_id'].nunique()

# Instanciando um objeto no grafo
G = nx.Graph()

G.add_nodes_from(df_recomend['id'].unique(),node_type='restaurant')
G.add_nodes_from(df_recomend['user_id'].unique(),node_type='user')
G.add_weighted_edges_from(df_recomend[['id','user_id','review_rate']].values)

G.number_of_nodes(),G.number_of_edges()

def pesquisa_restaurante():
  restaurant_name = input(str('Qual o nome do restaurante que você gosta?'))
  restaurant_id = df_recomend[df_recomend['name'] == restaurant_name]['id'].unique()[0]
  neighbor = G.neighbors(restaurant_id)

  user_consumed_rest = []

  for user_id in neighbor:
    user_consumed = G.neighbors(user_id)
    user_consumed_rest += user_consumed
    names_user_consumed_rest  = df_recomend[df_recomend['id'].isin(user_consumed_rest)]['name'].unique().tolist()
    names_user_consumed_rest.remove(restaurant_name)

  print(f'As pessoas que gostam de {restaurant_name} costumam gostar de {names_user_consumed_rest}')

  # Contagem de restaurantes para Score
  restaurant_count = Counter(user_consumed_rest)

  # Transformando em uma lista de itens únicos
  restaurant_count_list = list(set(list(restaurant_count.elements())))

  # Filtrando os nomes de cada restaurante no dataset a partir da lista encontrada
  names_count = df_recomend[df_recomend['id'].isin(restaurant_count_list)]['name'].unique().tolist()


 # Finalmente, decidindo a lista final dos restaurantes com suas respectivas contagens
  Score_restaurants = list(zip(names_count, restaurant_count.values()))

  print(f'A contagem dos restaurantes é {Score_restaurants}')

  return

pesquisa_restaurante()

### KNN

# Serial

pickle.dump(G, open("grafo_modelo.pkl", "wb"))
pickle.dump(df_recomend, open("df_recomend.pkl", "wb"))